### Pig Lab
### CSCE 587
### Cort Miles

Commands listed below:

#Load data
wget https://cse.sc.edu/~rose/587/CSV/drivers.csv --no-check-certificate            
wget https://cse.sc.edu/~rose/587/CSV/timesheet.csv --no-check-certificate   

#peek at the data
cat drivers.csv
cat timesheet.csv
wc timesheet.csv

#Now we want to transfer the data to hadoop
hadoop fs -put drivers.csv /user/maria_dev
hadoop fs -put timesheet.csv /user/maria_dev

#invoke grunt (pig interactive env)
pig

# determine the working dir
pwd

#ensure the files we need are there
ls

#load the first file into drivers
drivers = LOAD 'drivers.csv' USING PigStorage(',');

#look at the data
dump drivers

#add this line of code (it removes the first line which has the column heading info)
raw_drivers = FILTER drivers BY $0>1;

#look at changed drivers
dump raw_drivers

# create new obj consisting of the first two col of raw_drivers data
drivers_details = FOREACH raw_drivers GENERATE $0 AS driverId, $1 AS name;

#look at resulting table (col1 is driverID and col2 is driversName)
dump drivers_details

#load the second datasheet, timesheet.
timesheet = LOAD 'timesheet.csv' USING PigStorage(',');

#look at the the data
dump timesheet

#filter first row of headings
raw_timesheet = FILTER timesheet by $0>1;

#select subset of column data
timesheet_logged = FOREACH raw_timesheet GENERATE $0 AS driverId, $2 AS hours_logged, $3 AS miles_logged;

#look at the data
dump timesheet_logged

#group all rows for a given driverID
grp_logged = GROUP timesheet_logged by driverId;

#look at the data
dump grp_logged

#create sum_logged from grp_logged
sum_logged = FOREACH grp_logged GENERATE group as driverId,
SUM(timesheet_logged.hours_logged) as sum_hourslogged,
SUM(timesheet_logged.miles_logged) as sum_mileslogged;

		#####NOTE Where do we get timesheet_logged.hours_logged & timesheet_logged.miles_logged?
		We created them earlier with this command:
		timesheet_logged = FOREACH raw_timesheet GENERATE $0 AS driverId, $2 AS hours_logged, $3 AS miles_logged;

#look at the data
dump sum_logged

#join the sum_logged and drivers_details tables by matching rows that have the same driverId.
join_sum_logged = JOIN sum_logged by driverId, drivers_details by driverId;

#look at the data
dump join_sum_logged

#selecting col 1.2.3.5 from the join table join_sum_logged
join_data = FOREACH join_sum_logged GENERATE $0 as driverId, $4 as name, $1 as hours_logged, $2 as miles_logged;

#lets look again at the data
dump join_data 

#Save the data
STORE join_data INTO 'JD';

#find name of output 
ls JD

#exit pig and transfer to linux filesystem
^C (this didnt work so i used the command quit)

hadoop fs –get JD/part-v003-o000-r-00000



                                                      